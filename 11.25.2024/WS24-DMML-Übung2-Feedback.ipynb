{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMML 25.11.2024: Feedback Übung2 - Gewichtsklasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alle: Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Gitternetz</b>\n",
    "    <ul>\n",
    "    <li>Nur wenige von euch haben Diagramme mit Gitternetz gemacht: plt.grid()\n",
    "    <li>Aus meiner Sicht erleichert das i.d.R. das Lesen von Diagrammen</li>\n",
    "    </ul>\n",
    "<li><b>Positive Klasse (1.Zeile/1.Spalte Konfusionsmatrix)</b>\n",
    "    <ul>\n",
    "    <li>Die positive Klasse ist die Klasse, die vom größeren Interesse ist\n",
    "    <li>Diese sollte mit 0 kodiert werden (war leider in der Angabe anders!)\n",
    "    <li>Häufig ist die positive Klasse die Klasse zu der es weniger Datensätze gibt\n",
    "    </ul>\n",
    "<li><b>Konfidenzmatrix</b>\n",
    "    <ul>\n",
    "    <li>Diese sollte immer als Matrix dargestellt sein\n",
    "    <li>Sollte eine Methode keine Matrix liefern, ist ein anderer Weg zu suchen\n",
    "    </ul>\n",
    "<li><b>Präsentationen</b>\n",
    "<ul>\n",
    "<li>Zunächst die Aufgabenstellung erklären, dann<br>\n",
    "    (https://www.inc.com/john-baldoni/deliver-a-great-speech-aristotle-three-tips.html)\n",
    "    <ol>\n",
    "    <li>Tell them what you will tell them\n",
    "    <li>Tell them\n",
    "    <li>Tell them what you just told them\n",
    "    </ol>\n",
    "</ul>\n",
    "<li><b>Dezimaltrennzeichen Komma bei csv-Dateien</b>\n",
    "    <ul>\n",
    "    <li>In R gibt es \"read.csv\" und \"read.csv2\"\n",
    "    <li>Bei Pandas gibt es bei pd.read_csv die Parameter sep (; oder ,) und decimal (, oder .)\n",
    "    </ul>\n",
    "<li><b>Runden Kennwerte</b><br>\n",
    "    <ul>\n",
    "    <li>Beim Data Science geht es idR um das Schätzen von Parametern bei Stichproben\n",
    "    <li>Die Darstellung von 16 Nachkommastellen ist eine Darstellung einer Scheingenauigkeit\n",
    "    <li>Bei uns reichen idR eine Darstellung von 2-3 Ziffern (die ungleich 0 sind) aus!\n",
    "    </ul>\n",
    "<li><b>Warnmeldungen - vor allem Futurewarning</b><br>\n",
    "    <ul>\n",
    "    <li>Der Code sollte so angepasst werden, dass keine Futurewarnings angezeigt werden\n",
    "    <li>Dadurch wird sichergestellt, dass der Code etwas länger mit neuen Versionen läuft       \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alle: pd.read_csv - Spaltentrennzeichen und Dezimaltrennzeichen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Bei pd.read_csv gibt es den Parameter \"decimal\" zur Festlegung des Dezimaltrennzeichens.<br>\n",
    "Vorteil: Weniger Code + Variable erhalten den Datentyp \"float\" und nicht \"object\" -<br>\n",
    "im Vergleich zum Import mit falschem Dezimaltrennzeichen, und einem Ersetzen danach.</b><br>\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Gewichtsklasse.csv\", sep=';', decimal=',')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alle: Codierung - Übersicht Codealternativen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Label encoding</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Fastfood'\n",
    "print('Vorher: ', df.dtypes[col], list(df[col][0:10]) )\n",
    "df[col] = df[col].map({'ja': 1, 'nein': 0}) # bei 'vielleicht' würde ein nan entstehen\n",
    "print('Nachhher: ', df.dtypes[col], list(df[col][0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Geschlecht'\n",
    "print('Vorher: ',df.dtypes[col], list(df[col][0:10]))\n",
    "df[col]  = LabelEncoder().fit_transform(df[col]) # bei einem dritten Wert würde auf 0, 1 und 2 codiert werden\n",
    "print('Nachher: ',df.dtypes[col], list(df[col][0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Transportmittel'\n",
    "print('Vorher: ',df.dtypes[col], list(df[col][0:10]))\n",
    "df[col] = LabelEncoder().fit_transform(df[col])\n",
    "print('Nachher: ',df.dtypes[col], list(df[col][0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Alkohol'\n",
    "print('Vorher: ', df.dtypes[col], list(df[col][0:10]) )\n",
    "df[col] = df[col].replace({'nie': 0, 'manchmal': 1, 'häufig': 2, 'täglich':3})\n",
    "print('Nachhher: ', df.dtypes[col], list(df[col][0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>One hot encoding, dummy encoding</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Zwischenmahlzeit'\n",
    "print('Vorher: ',df.dtypes[col], list(df[col][10:20]))\n",
    "df = pd.get_dummies(df, columns=[col], drop_first=False)\n",
    "cols = list(df.columns[df.columns.str.contains('Zwischen')])\n",
    "print('Nachher: ')\n",
    "print('\\n', df[cols].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gruppe 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Bei den Histogrammen - gute Wahl der Anzahl bins. Schöne Korrelationsmatrix.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tipp:<br>bei der Pandasmethode corr() kann direkt auf numerische Variable eingeschränkt werden:<br> df.corr(method='pearson', numeric_only=True)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G1: Korrelation numerischer und binäre Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Wichtigste Punkte\n",
    "<ul>\n",
    "<li><b>Die Formel der Pearson-Korrelation vereinfacht sich hier, und wird als <b>Punktbiseriale Korrelation</b> bezeichnet.</b>\n",
    "<li>Die Korrelation drückt aus, in welche Richtung sich der Mittelwert der numerischen Variablen von der Kodierung \"0\" zur Kodierung \"1\" ändert.\n",
    "<li>Der Betrag der Korrelation wird kleiner, je unbalancierter (bei konstanten Mittelwerten pro Gruppe und bei konstanter Standardabweichung) die Datensätze bzgl. der Binärvariablen sind.\n",
    "<li>Der Betrag der Korrelation wird kleiner, je größer die Standardabweichung des Datensatzes ist (bei konstanten Mittelwerten pro Gruppe, und konstanter Balanciertheit)    \n",
    "</ul></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G1: Korrelation zwischen zwei Binärvariablen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Die Formel der Pearson-Korrelation vereinfacht sich hier, und wird als <b>Phi-Koeffizient</b> bezeichnet.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Rauchen  = df.Rauchen.map({'ja': 1, 'nein': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstable = pd.crosstab(df.Rauchen, df.Fastfood, margins=False, margins_name=\"Gesamt\")\n",
    "crosstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = ( (crosstable.iloc[0, 0] * crosstable.iloc[1, 1] - crosstable.iloc[0, 1] * crosstable.iloc[1, 0]) /\n",
    "        (np.sqrt(((crosstable.iloc[0, 0] + crosstable.iloc[0, 1]) * (crosstable.iloc[1, 0] + crosstable.iloc[1, 1]) *      \n",
    "                  (crosstable.iloc[0, 0] + crosstable.iloc[1, 0]) * (crosstable.iloc[0, 1] + crosstable.iloc[1, 1]))))   )\n",
    "print(f\"Phi-Koeffizient: {phi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Bemerkung: eure Formel Phi-Koeffizient war falsch. Ihr hattet den Zähler quadrariert, dann die Wurzel gezogen, und dadurch die Wurzel zerstört.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\n",
    "df[['Rauchen','Fastfood']].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Korrelation zwischen ordinalskalierten und numerischen Variablen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Die Verwendung der Pearson-Korrelation ist bei ordinalen Variablen \"mathematisch\" nicht korrekt</b>\n",
    "<li>Es ist der Rangkorrelationskoeffizient (Spearman) zu verwenden.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Alkohol', 'Gewicht']].corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Alkohol', 'Gewicht']].corr(method = 'spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gruppe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Tipp: Um True/False in 0/1 umzuwandeln reicht: df = df *1</b>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gruppe 3 - kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>k = 2</b><br>\n",
    "    Bei scikit-learn wird bei 2 Nachbarn als Tiebraeker die Klasse des ersten Nachbarn im Trainingssatz genommen.<br>\n",
    "https://scikit-learn.org/dev/modules/generated/sklearn.neighbors.KNeighborsClassifier.html<br>\n",
    "Das ist sicher nicht optimal.\n",
    "<li>Beim Wertebereich von k sollte statt: <b>k_values = range(2,50)</b> besser <b>k_values = range(1,51)</b> gewählt werden.<br>\n",
    "<b>Bei mir wurde k=1 wurde als optimaler k-Wert erkannt!</b>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Gewichtsklasse.csv\", delimiter=';', decimal = ',')\n",
    "label_encoder = LabelEncoder()\n",
    "cols2decode = ['Geschlecht', 'Alkohol', 'Fastfood', 'Kalorienzählen', 'Rauchen', 'Familie','Zwischenmahlzeit', 'Transportmittel']\n",
    "df_dec = pd.get_dummies(df, columns=cols2decode, drop_first=False) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dec['Target'] = df_dec.Gewichtsklasse.apply(lambda x: 1 if 'Adipositas' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split encoded dataframe into train- and test-dataframe\n",
    "X = df_dec.drop(columns=['Target', 'Gewichtsklasse', 'Gewicht'])\n",
    "y = df_dec['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data for kNN\n",
    "scaler = StandardScaler()\n",
    "X_train_z = scaler.fit_transform(X_train)\n",
    "X_test_z = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "k_values = range(1,20)\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    scores = cross_val_score(knn, X_train_z, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# used code from Jupyter Notebook \"kNN_Example\" Jupyter Notebook by Alexander Adrowitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_acc = pd.DataFrame({'k': k_values, 'Acc': cv_scores})\n",
    "k_acc[k_acc['Acc'] == max(k_acc['Acc'])]\n",
    "\n",
    "# used code from Jupyter Notebook \"kNN_Example\" Jupyter Notebook by Alexander Adrowitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy classifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "# train model\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# make prediction\n",
    "y_train_pred = dummy_clf.predict(X_train)\n",
    "y_test_pred = dummy_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2,p=2,metric='minkowski')\n",
    "knn.fit(X_train_z, y_train)\n",
    "y_train_pred = knn.predict(X_train_z)\n",
    "y_test_pred = knn.predict(X_test_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Einschub ROC-Kurve (TPR über FPR)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.evidentlyai.com/classification-metrics/explain-roc-curve#:~:text=The%20ROC%20curve%20stands%20for,False%20Positive%20rates%20(FPR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>kNN(k=2)- mögliche Fälle bei berechneten Wahrscheinlichkeit des Klassifikators</b>\n",
    "<ul>\n",
    "<li>Wahrscheinlichkeit(positiv) = wie sicher der Klassifikator ist, dass die Klasse positiv ist\n",
    "<li>wenn beide nächste Nachbarn des Test-Datenpunkts positiv => Wahrscheinlichkeit(positiv) = 1\n",
    "<li>wenn beide nächste Nachbarn des Test-Datenpunkts negativ => Wahrscheinlichkeit(positiv) = 0\n",
    "<li>wenn beide nächste Nachbarn des Test-Datenpunkts pos. bzw. neg. => Wahrscheinlichkeit(positiv) = 0.5\n",
    "</ul>\n",
    "<b>Da bedeutet für die ROC-Kurve:</b>\n",
    "<ul>\n",
    "<li>bei der Abfrage <b>knn.predict_proba(X_test_z)</b> gibt es nur Werte 0, 0.5, 1\n",
    "<li>Idee Threshold: Wahrscheinlichkeit(positiv) >= TH => Prediction = positiv\n",
    "<li>TH ist ein Parameter, der nach unseren Wünschen festgelegt werden kann!\n",
    "<li>TH = -1.0 => alle Datenpunkte werden positiv geschätzt\n",
    "<li>TH =  0.2 => nur Datenpunkte mit P(p) = 0.5 oder 1 werden positiv geschätzt\n",
    "<li>TH =  0.7 => nur Datenpunkte mit P(p) = 1 werden positiv geschätzt\n",
    "<li>TH =  1.4 => alle Datenpunkte werden negativ geschätzt\n",
    "</li>\n",
    "=> im ROC gibt es nur 4 verschiedene Punkte beim kNN Classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# die Wahrscheinlichkeiten der ersten 10 Testdatensätze: dummy und kNN\n",
    "print(dummy_clf.predict_proba(X_test_z)[0:10, 1])\n",
    "print(knn.predict_proba(X_test_z)[0:10, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung FRP und TPR für Dummy und kNN\n",
    "fpr_test_dummy, tpr_test_dummy, _ = roc_curve(y_test, dummy_clf.predict_proba(X_test)[:,1])\n",
    "fpr_test_knn, tpr_test_knn, _     = roc_curve(y_test, knn.predict_proba(X_test_z)[:,1])\n",
    "\n",
    "print(fpr_test_dummy)\n",
    "print(tpr_test_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ROC curve\n",
    "# https://scikit-learn.org/dev/modules/generated/sklearn.metrics.roc_curve.html\n",
    "\n",
    "plt.plot(fpr_knn_test, tpr_knn_test, label='kNN Classifier',marker='o')\n",
    "plt.plot(fpr_dummy, tpr_dummy,marker='o')\n",
    "plt.xlabel('FPR (False Positive Rate)')\n",
    "plt.ylabel('TPR (True Positive Rate)')\n",
    "plt.title('ROC curve (Test)')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_dummy, tpr_dummy, _ = roc_curve(y_train, dummy_clf.predict_proba(X_train)[:,1])\n",
    "fpr_knn_train, tpr_knn_train, _ = roc_curve(y_train, knn.predict_proba(X_train_z)[:,1])\n",
    "plt.plot(fpr_knn_train, tpr_knn_train, label='kNN Classifier',marker='o')\n",
    "plt.plot(fpr_dummy, tpr_dummy, marker='o')\n",
    "plt.xlabel('FPR (False Positive Rate)')\n",
    "plt.ylabel('TPR (True Positive Rate)')\n",
    "plt.title('ROC curve (Training)')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Warum ist ROC Kurve hier mit kNN bei den Trainingsdaten so viel besser?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gruppe 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>alles OK</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Ergebnis:<br>\n",
    "Bei dieser einfachen Versuchsanordnung war es für die Accuracy egal, ob die ordinalen Variablen ordinal codiert oder dummy codiert wurden.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gruppe 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Kein Feedback, da keine Abgabe</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gruppe 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gruppe hat Aufgabe 6 (statt 7) bearbeitet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Korrelationsmatrix</b>\n",
    "    <ul>\n",
    "    <li>Gerade bei Korrelationsmatrizen mit negativen und positiven Werten sollte eine Korrelation 0 farblich schnell entdeckt werden (weiß).\n",
    "    <li>sns.heatmap(correlation_matrix, vmin=-1, vmax=1, center=0, ....\n",
    "    </ul>\n",
    "<li><b>Kopieren DataFrame</b>\n",
    "    <ul>\n",
    "    <li>df_encoded = df.copy()\n",
    "    <li><b>Mögliche sinnvolle Strategie:</b><br>\n",
    "        df sollte möglichst nah bei den Rohdaten bleiben, da man dann immer darauf zurückgreifen kann, ohne neu zu laden<br>\n",
    "        Bei verschiedenen Änderungen an df jeweils eine neue (sprechende) Bezeichnung verwenden\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dieser Code ist hier nicht lauffähig!!!\n",
    "'''\n",
    "binaer_yes_no_features = ['Fastfood', 'Kalorienzählen', 'Rauchen', 'Familie']\n",
    "\n",
    "# Diese Schleife ist nicht nötig:\n",
    "for e in binaer_yes_no_features:\n",
    "    df_encoded[e] = df_encoded[e].replace({'ja': 1, 'nein': 0})\n",
    "\n",
    "# Das geht so ohne Schleife:\n",
    "df_encoded[binaer_yes_no_feature] = df_encoded[binaer_yes_no_feature].replace({'ja': 1, 'nein': 0})\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Zusatzbemerkung zu randomizedsearchcv() (für alle!)<br>\n",
    "Es ist möglich, Hyperparameter zufällig aus Wahrscheinlichkeitsverteilungen zu ziehen (bei Gridsearch nicht möglich)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "param_dist = {\n",
    "   'C':      rd.uniform(0.1, 10),  # Uniform distribution between 0.1 and 10\n",
    "   'kernel': ['linear', 'rbf', 'poly'],\n",
    "   'gamma':  ['scale', 'auto'] + list(np.logspace(-3, 3, 50))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature Importances beim Random Forest</b><br>\n",
    "https://scikit-learn.org/1.5/auto_examples/ensemble/plot_forest_importances.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train = rf.predict(X_train)\n",
    "y_pred_test  = rf.predict(X_test)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "print('Accuracy(test)= ',round(acc_test*100,1),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns\n",
    "importances = rf.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)   # Standardabweichung der Importances pro Baum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title('Feature importances using mean decrease impurity (MDI)')\n",
    "ax.set_ylabel('MDI')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 7 PCA - wurde nicht vorgestellt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/dev/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Standardsierung - was macht die Methode PCA() ?</b><br>\n",
    "\n",
    "<b>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.<br>\n",
    "The input data is <u>centered but not scaled</u> for each feature before applying the SVD.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Standardsierung - was sollte man machen?</b><br>\n",
    "https://medium.com/@roshmitadey/understanding-principal-component-analysis-pca-d4bb40e12d33#:~:text=Before%20performing%20PCA%2C%20it's%20essential,equal%20importance%20in%20the%20analysis.\n",
    "Before performing PCA, it’s essential to standardize the data.<br>\n",
    "This means <b>centering the data by subtracting the mean and scaling it by dividing by the standard deviation</b>.<br>\n",
    "Standardization ensures that all features have equal importance in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stand = scaler.fit_transform(X_train)\n",
    "X_test_stand  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir haben 31 features => maximal 30 Hauptkomonenten sind erzeugbar!\n",
    "pca         = PCA(n_components = X_train_stand.shape[1])\n",
    "X_train_pca = pca.fit_transform(X_train_stand)\n",
    "X_test_pca  = pca.transform(X_test_stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Eigenschaften der Hauptkomponenten</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diese sind mittelwertfrei\n",
    "print(X_train_pca[:,0].mean())\n",
    "print(X_train_pca[:,1].mean())\n",
    "print(X_train_pca[:,2].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# die Varianz der einzelnen Hauptkomponenten nimmt ab\n",
    "print(round(np.var(X_train_pca[:,0]),2))\n",
    "print(round(np.var(X_train_pca[:,1]),2))\n",
    "print(round(np.var(X_train_pca[:,2]),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# die einzelnen Hauptkomponenten stehen senkrecht aufeinander (Skalarprodukt der Vektoren ist null)\n",
    "print(np.dot(X_train_pca[:,0],X_train_pca[:,1]))\n",
    "print(np.dot(X_train_pca[:,0],X_train_pca[:,2]))\n",
    "print(np.dot(X_train_pca[:,1],X_train_pca[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# der Korrelationskoeffizent zwischen den Hauptkomponenten ist dann natürlich auch 0\n",
    "print(np.corrcoef(X_train_pca[:,0],X_train_pca[:,1])[0,1])\n",
    "print(np.corrcoef(X_train_pca[:,0],X_train_pca[:,2])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c= y_train, cmap='plasma')\n",
    "plt.xlabel('Erste Hauptkomponente')\n",
    "plt.ylabel('Zweite Hauptkomponente')\n",
    "plt.title('Klassenverteilung der ersten beiden Hauptkomponenten')\n",
    "plt.legend(['adipös','nicht adipös']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SVC + PCA</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC ohne PCA zum vergleichen\n",
    "svc = svm.SVC() # alles default\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred_test  = svc.predict(X_test)\n",
    "acc_test     = accuracy_score(y_test, y_pred_test)\n",
    "print('Accuracy(test)= ',round(acc_test*100,1),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sammlung Accuracy für 1 bis 31 Hauptkomponenten\n",
    "acc_test = list()\n",
    "\n",
    "for i in range(1, X_train_stand.shape[1]+1):\n",
    "    svc.fit(X_train_pca[:,0:i], y_train)\n",
    "    y_pred_test  = svc.predict(X_test_pca[:,0:i])\n",
    "    y_pred_train = svc.predict(X_train_pca[:,0:i])\n",
    "    acc_test.append(accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, X_train_stand.shape[1]+1),acc_test, marker='o')\n",
    "plt.title('Accuracy über Anzahl Hauptkomponenten (SVC)')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC ohne PCA\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred_test  = svc.predict(X_test)\n",
    "print('Confusionmatrix(Test)\\n ',confusion_matrix(y_test, y_pred_test))\n",
    "print('Accuray(Test) ',round(accuracy_score(y_test, y_pred_test),3))\n",
    "round(acc_test,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Hauptkomponenten\n",
    "svc.fit(X_train_pca[:,0:5], y_train)\n",
    "y_pred_test  = svc.predict(X_test_pca[:,0:5])\n",
    "print('Confusionmatrix(Test)\\n ',confusion_matrix(y_test, y_pred_test))\n",
    "print('Accuray(Test) ',round(accuracy_score(y_test, y_pred_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22 Hauptkomponenten\n",
    "svc.fit(X_train_pca[:,0:22], y_train)\n",
    "y_pred_test  = svc.predict(X_test_pca[:,0:22])\n",
    "print('Confusionmatrix(Test)\\n ',confusion_matrix(y_test, y_pred_test))\n",
    "print('Accuray(Test) ',round(accuracy_score(y_test, y_pred_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gruppe 7 (Voting, Bagging, Pasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Alles OK</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übersicht Ergebnisse Binäre Klassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{|c|c|} \\hline\n",
    "\\text{Klassifizierer} & \\text{Accuracy} \\\\ \\hline \\hline\n",
    "\\text{Dummy} & 55\\% \\\\ \\hline\n",
    "\\text{SVC} & 76\\%  \\\\ \\hline\n",
    "\\text{Log. Regression} & 79\\%  \\\\ \\hline\n",
    "\\text{Decision tree} & 88\\%  \\\\ \\hline\n",
    "\\text{SVC+PCA} & 89\\%  \\\\ \\hline\n",
    "\\text{kNN (k=5)} & 92\\%  \\\\ \\hline\n",
    "\\text{Decision tree (pasting)} & 92\\%  \\\\ \\hline\n",
    "\\text{Decision tree (bagging)} & 94\\%  \\\\ \\hline\n",
    "\\text{Random Forest} & 94\\%  \\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Für vertrauenswürdigere Ergebnisse sind Hyperparameteroptimierungen, Feature Selection und mehrfache Train-Test-Splits vorzunehmen.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gruppe 8 (Mehrklassen-Klassifizierung)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Korrelationsmatrix</b>\n",
    "    <ul>\n",
    "    <li>Gerade bei Korrelationsmatrizen mit negativen und positiven Werten sollte eine Korrelation 0 farblich schnell entdeckt werden (weiß).\n",
    "    <li>sns.heatmap(correlation_matrix, vmin=-1, vmax=1, center=0, ....\n",
    "    </li>\n",
    "    </ul>\n",
    "<li><b>ROC im Multiklassen-Fall</b>\n",
    "    <ul>\n",
    "    <li>Auch in diesem Fall ist es möglich, ROCs zu erzeugen, siehe<br>\n",
    "        https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gruppe 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Grafiken</b>\n",
    "    <ul>\n",
    "    <li>Bei den Histogrammen sind die Anzahl Bins zu klein\n",
    "    <li>Beschriftungen der x-Achse überlappen sich\n",
    "    <li>Gut: Verwendung subplots\n",
    "    </ul>\n",
    "<li><b>Codierung</b>\n",
    "    <ul>\n",
    "    <li>Schleifen über Spalten sind im Pandas nicht nötig\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gruppe 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>Codierung der Gewichtsklasse ist unnötig</b>\n",
    "<li><b>Schleifen über Spalten</b>\n",
    "    <ul>\n",
    "    <li>Schleifen über Spalten sind im Pandas nicht nötig\n",
    "    </li>\n",
    "    </ul>\n",
    "<li><b>Verwendung Spalten</b>\n",
    "    <ul>\n",
    "    <li>Spalten 'Alkohol', 'Zwischenmahlzeit', 'Transportmittel' wurden ohne Begründung gelöscht.<br>\n",
    "        Löschen von Daten ist immer zu begründen.\n",
    "    </ul>\n",
    "<li><b>Codeverweise</b>\n",
    "    <ul>\n",
    "    <li>Ich denke, die Funktionen habt ihr nicht selbst erstellt, dh Verweis fehlt!\n",
    "    <li>Aber schöne Grafiken\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Wie ihr das im Folgenden gemacht habt, sollte das NICHT gemacht werden:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sklearn_lda = LDA()                                    \n",
    "X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda_sklearn, y, random_state=147, test_size = 0.2, shuffle=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Auf diese Weise sollte das geschehen:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_z = sc.fit_transform(X_train)\n",
    "X_test_z  = sc.transform(X_test)\n",
    "\n",
    "lda = LDA()\n",
    "X_train_lda = lda.fit_transform(X_train_z, y_train_z)\n",
    "X_test_lda  = lda.transform(X_test_z)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>range(1,6) erzeugt Integer von 1-5 - daher fehlt 6. Komponente.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1, 6):\n",
    "    print(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
