{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where SVMs become really powerful is when it is combined with so called _kernels_. This kernels allow us to find linear decision boundaries in higher dimensions when in the original dimension no linear boundary can be found.\n",
    "\n",
    "We will not discussed exactly on how the Support Vector Machine is computed, because the details become somewhat technical. However, it turns out that the solution involves only the _inner products_ (_dot products_) of the observations as opposed to the observations themselves. The inner product of two observations $x_i$ and $x_{i'}$ is given by\n",
    "\n",
    "$$\n",
    "\\langle x_i,x_{i'} \\rangle = \\sum^p_{j=1}x_{ij}x_{i'j}\n",
    "$$\n",
    "\n",
    "which in form of the linear classifier $f(x)$ can be represented as\n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\sum^n_{i=1}\\alpha_i \\langle x,x_i \\rangle\n",
    "$$\n",
    "\n",
    "where there are n parameters $\\alpha_i$, $i = 1,...,n$, one per training observation.\n",
    "\n",
    "To estimate the parameters $\\alpha_1,...,\\alpha_n$ and $\\beta_0$, all we need are the $\\binom{n}{2}$ inner products $\\langle x_i,x_{i'} \\rangle$ between all pairs of training observations.\n",
    "\n",
    "In order to evaluate the function $f(x)$, we need to calculate the inner product between the new point $x$ and each of the training points $x_i$. However it turns out that $\\alpha_i$ is nonzero for the support vectors only. If the training observation is not a support vector, $\\alpha_i$ turns exactly zero. To sum up, we define a collection $S$ with indices of these support points, therefore we can rewrite any solution function of the form as\n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\sum_{i \\in S}\\alpha_i \\langle x,x_i \\rangle\n",
    "$$\n",
    "\n",
    "which obviously contains much fewer terms than all the training observations.\n",
    "\n",
    "To demonstrate how kernels work, we begin with the imports we need to provide some examples and define our function to plot the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "plt.rcParams['image.cmap'] = 'Dark2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Kernel\n",
    "\n",
    "Now suppose that every time the inner product appears in the calculation of the solution for the support vector classifier, we can replace it with a generalization of the form\n",
    "\n",
    "$$\n",
    "K(x_i,x_{i'})\n",
    "$$\n",
    "\n",
    "where $K$ is some function we refer to as _kernel_. We already discussed the linear kernel which simply put gives as back the Support Vector Classifier\n",
    "\n",
    "$$\n",
    "K(x_i,x_{i'}) = \\beta_0 + \\sum^p_{j=1}x_{ij}x_{i'j}\n",
    "$$\n",
    "\n",
    "The linear kernel essentially quantifies the similarity between two observations using Pearson (standard) correlation. The linear kernel describes a linear relationship between to observations to quantify if a obersavtion is more similar to one or the other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax = None, plot_support = True):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "\n",
    "    ax.contour(X, Y, P, colors = 'k', levels = [-1, 0, 1], alpha = 0.5, linestyles=['--', '-', '--'])\n",
    "    \n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s = 300, lw = 1, facecolors = 'red', alpha = 0.2);\n",
    "        \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "X, y = make_blobs(n_samples = 30, centers = 2, random_state = 0, cluster_std = 0.6)\n",
    "\n",
    "model = SVC(kernel = 'linear', C = 1E10)\n",
    "model.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, s = 40)\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel\n",
    "\n",
    "More often than not, our data is not separatable with a linear decision boundary.<br>\n",
    "Let's take a look at the following data set which consists observations of drug usages to cure a sickness.\n",
    "\n",
    "The dosage of the drug determines if the patient got cured (green) or is still sick (black). After taking a look at the observations we can clearly see that if the taken dosage is too little or too much the disease was not cured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('drug_dosages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['dosage'].values\n",
    "y = np.zeros(X.shape)\n",
    "colors = np.where(df['status'] == 'sick', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7, 2))\n",
    "\n",
    "plt.xlim(0, 29)\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.yticks([])\n",
    "\n",
    "plt.scatter(X, y, c = colors, s = 40)\n",
    "plt.xlabel('\\ndosage (mg)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This introduces a problem because this classes are not linear separatable, but what if we transform this data points to another dimension?\n",
    "\n",
    "This trick would help us to draw a linear decision boundary on a higher dimension. Consider a transformation of the data into the two-dimensional space where we can compare the similarities of the observations using the polynomial kernel\n",
    "\n",
    "$$\n",
    "K(x_i,x_{i'}) = (\\beta_0 + \\sum^p_{j=1}x_{ij}x_{i'j})^d\n",
    "$$\n",
    "\n",
    "with a degree of $d$, where $d$ is a positive integer. Using this kernel leads to a much more flexible decision boundary by fitting a Support Vector Classifier in a higher dimensional space involving polynomials rather than in the original feature space.\n",
    "\n",
    "When the Support Vector Classifier is combined with a non-linear kernel such as the polinomial kernel, the resulting classifier is known as the Support Vector Machine. In this example, the (non-linear) function has the form of\n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\sum_{i \\in S} \\alpha_i K(x, x_i)\n",
    "$$\n",
    "\n",
    "where $\\alpha_i$ defines the slope and $\\beta_0$ the constant term.\n",
    "\n",
    "To demonstrate, we choose a degree of $d = 2$, a slope $\\alpha = 1$ and a coefficient of $\\beta_0 = 1$ and now quantify the similarity of two observations $x_i$ and $x_{i'}$. Our polynomial kernel function is now defined as\n",
    "\n",
    "$$\n",
    "K(x_i,x_{i'}) = (1 + \\sum^p_{j=1}x_{ij}x_{i'j})^2\n",
    "$$\n",
    "\n",
    "To simplify the process, let's quantify the similarity of two observations $x_{i1} = a$ and $x_{i'1} = b$, which inserted in our kernel look like the following equation\n",
    "\n",
    "\\begin{align}\n",
    "K(a,b) &= (1 + a \\times b)^2\\\\\n",
    "&= (1 + a \\times b)(1 + a \\times b)\\\\\n",
    "&= 2ab + a^2b^2 + 1\\\\\n",
    "&= (\\sqrt{2}a, a^2, 1) \\cdot (\\sqrt{2}b, b^2, 1)\n",
    "\\end{align}\n",
    "\n",
    "This dot product describes our data points in the two-dimensional space, where the first term describes the x-value, the second term the y-value and the third-term the z-value. Therefore our new x-values move with a factor of $\\sqrt{2}$ down the x-axis, the y-value is simply the original x-value quared ($x^2$) and Our z-value is a constant term of one.\n",
    "\n",
    "With this trick, the calculated dot product describes our data points in all dimensions without the need to transform the data into other dimensions. We refer to this trick as _kernel trick_.\n",
    "\n",
    "To demonstrate, our transformed data points can be displayed on a two-dimensional space looking like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X * np.sqrt(2)\n",
    "y = X ** 2\n",
    "\n",
    "plt.scatter(X, y, c = colors, s = 40)\n",
    "plt.xlabel('\\ndosage (mg)')\n",
    "plt.ylabel('\\nsquared dosage (mg)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate, we can now fit a SVM again to show the (linear) decision boundary, which will be a straight line.\n",
    "\n",
    "Please note two things:\n",
    "* The SVM does NOT transform data into other dimensions, it simply calculates the similaritie between observation points using higher dimensions\n",
    "* The SVM will NOT fit a linear kernel after the transformation, so the following graph is only for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.reshape(X, (-1, 1))\n",
    "b = np.reshape(y, (-1, 1))\n",
    "c = np.append(a, b, axis = 1)\n",
    "\n",
    "model = SVC(kernel = 'linear', C = 1E10).fit(c, colors)\n",
    "\n",
    "plt.scatter(a[:, 0], b[:, 0], c = colors, s = 40)\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the SVM calculates the similarities between observations and therefor can draw a polynomial decision boundary to separate between the two classes.\n",
    "\n",
    "When we now fit a SVM using a polynomial kernel, the decision boundary looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df['dosage'].values\n",
    "X = np.append(np.reshape(X, (-1, 1)), np.reshape(np.ones(X.shape), (-1, 1)), axis = 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14, 5))\n",
    "fig.subplots_adjust(left = 0.0625, right = 0.95, wspace = 0.1)\n",
    "\n",
    "for axi, C in zip(ax, [1E-1, 1E2]):\n",
    "    model = SVC(kernel = 'poly', degree = 2, C = C, gamma = 1).fit(X, colors)\n",
    "    axi.set_xlim(0, 29)\n",
    "    axi.set_ylim(0, 2)\n",
    "    y_axis = axi.get_yaxis()\n",
    "    y_axis.set_visible(False)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c = colors, s = 40)\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Kernel (Gaussian RBF Kernel)\n",
    "\n",
    "The polynomial kernel as shown above is one example of a vast number of non-linear kernels. Another popular choice is the _radial kernel_ using the radial basis function, which takes the form of\n",
    "\n",
    "$$\n",
    "K(x_i,x_{i'}) = exp(-\\gamma\\sum^p_{j=1}(x_{ij}-x_{i'j})^2)\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is a positive constant.\n",
    "\n",
    "Alternatively the radial kernel could be implemented using\n",
    "\n",
    "$$\n",
    "K(x_i,x_{i'}) = exp(-\\frac{\\sum^p_{j=1}(x_{ij}-x_{i'j})^2}{2\\sigma^2})\n",
    "$$\n",
    "\n",
    "with $\\sigma$ as a tuning parameter, which plays a major role in the performance of the kernel. If overestimated, the exponential will behave almost linearly and the higher-dimension projection will start to loose its non-linear nature. On the other hand, if $\\sigma$ gets unterestimated, the function will lack regularization and the decision boundary will be highly sensitive to noise in the training data.\n",
    "\n",
    "Now let's take a look at our next data set, where the radial kernel would be the best fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(100, factor = .1, noise = .1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, s = 40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show, how a transformation to other dimensions might look like and to simplify the process again, we define $\\gamma = \\frac{1}{2}$ and quantify the similarity of two observations $x_{i1} = a$ and $x_{i'1} = b$, which put in our kernel look like the following equation:\n",
    "\n",
    "\\begin{align}\n",
    "K(a,b) &= e^{-\\frac{1}{2}(a-b)^2}\\\\\n",
    "&= e^{-\\frac{1}{2}(a^2+b^2-2ab)}\\\\\n",
    "&= e^{-\\frac{1}{2}(a^2+b^2)}e^{-\\frac{1}{2}-2ab}\\\\\n",
    "&= e^{-\\frac{1}{2}(a^2+b^2)}e^{ab}\n",
    "\\end{align}\n",
    "\n",
    "To now proceed, we can make use of the Taylor Series Expansion\n",
    "\n",
    "$$\n",
    "\\sum^{\\infty}_{n=0}\\frac{f^{(n)}(a)}{n!}(x-a)^n\n",
    "$$\n",
    "\n",
    "and expand the last term of our kernel\n",
    "\n",
    "$$\n",
    "e^{ab} = 1+\\frac{1}{1!}ab+\\frac{1}{2!}(ab)^2+\\frac{1}{3!}(ab)^3+...+\\frac{1}{\\infty!}(ab)^{\\infty}\n",
    "$$\n",
    "\n",
    "To know find the inner product which describe the position of our observations again, let's revisit the polynomial kernel again and think of the inner product for an infinite number of dimensions:\n",
    "\n",
    "$$\n",
    "a^0b^0+a^1b^1+a^2b^2+...+a^{\\infty}b^{\\infty} = (1, a, a^2, ..., a^{\\infty})\\cdot(1, b, b^2, ..., b^{\\infty})\n",
    "$$\n",
    "\n",
    "One thing catches our eye immediately, because a polynomial kernel with\n",
    "* $\\beta_0 = 0$ and $d = 0$ equals $1$\n",
    "* $\\beta_0 = 0$ and $d = 1$ equals $ab$\n",
    "* $\\beta_0 = 0$ and $d = 2$ equals $(ab)^2$\n",
    "* ...\n",
    "* $\\beta_0 = 0$ and $d = \\infty$ equals $(ab)^{\\infty}$\n",
    "\n",
    "We can make use of this and with that in mind, we can easily find the dot product of $e^{ab}$ in our radial kernel:\n",
    "\n",
    "$$\n",
    "(1, \\sqrt{\\frac{1}{1!}a}, \\sqrt{\\frac{1}{2!}a^2}, \\sqrt{\\frac{1}{3!}a^3}, ..., \\sqrt{\\frac{1}{\\infty!}a^{\\infty}})\n",
    "\\cdot\n",
    "(1, \\sqrt{\\frac{1}{1!}b}, \\sqrt{\\frac{1}{2!}b^2}, \\sqrt{\\frac{1}{3!}b^3}, ..., \\sqrt{\\frac{1}{\\infty!}b^{\\infty}})\n",
    "$$\n",
    "\n",
    "Going back to our original radial kernel function, we can now plug in the inner product of $e^{ab}$ and our function is now equal to this term\n",
    "\n",
    "\\begin{align}\n",
    "K(a,b) &= e^{-\\frac{1}{2}(a-b)^2}\\\\\n",
    "&= e^{-\\frac{1}{2}(a^2+b^2)}\n",
    "[\n",
    "(1, \\sqrt{\\frac{1}{1!}a}, \\sqrt{\\frac{1}{2!}a^2}, \\sqrt{\\frac{1}{3!}a^3}, ..., \\sqrt{\\frac{1}{\\infty!}a^{\\infty}})\n",
    "\\cdot\n",
    "(1, \\sqrt{\\frac{1}{1!}b}, \\sqrt{\\frac{1}{2!}b^2}, \\sqrt{\\frac{1}{3!}b^3}, ..., \\sqrt{\\frac{1}{\\infty!}b^{\\infty}})\n",
    "]\n",
    "\\end{align}\n",
    "\n",
    "The next steps would involve expanding our first exponential function as a Taylor Series, finding the inner product and incorporating the result in our already found dot product. We will skip this steps for sake of demonstration purposes and to further simplify, we can now introduce a variable $s$ which equals the square root of the first term:\n",
    "\n",
    "$$\n",
    "s = \\sqrt{e^{-\\frac{1}{2}(a^2+b^2)}}\n",
    "$$\n",
    "\n",
    "We now can multiply our dot product with the new term $s$ and get our final dot product to quantify the similarities of our observations using the radial kernel in an infinite number of dimensions:\n",
    "\n",
    "\\begin{align}\n",
    "K(a,b) &= e^{-\\frac{1}{2}(a-b)^2}\\\\\n",
    "&= \n",
    "(s, s\\sqrt{\\frac{1}{1!}a}, s\\sqrt{\\frac{1}{2!}a^2}, s\\sqrt{\\frac{1}{3!}a^3}, ..., s\\sqrt{\\frac{1}{\\infty!}a^{\\infty}})\n",
    "\\cdot\n",
    "(s, s\\sqrt{\\frac{1}{1!}b}, s\\sqrt{\\frac{1}{2!}b^2}, s\\sqrt{\\frac{1}{3!}b^3}, ..., s\\sqrt{\\frac{1}{\\infty!}b^{\\infty}})\n",
    "\\end{align}\n",
    "\n",
    "To simplify on what's happening here, let us show the theoratical transformation with the radial kernen into the third dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.exp(-(np.sqrt(1/2 * X ** 2)).sum(axis = 1))\n",
    "\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "fig.set_facecolor('#EAEAF2')\n",
    "\n",
    "ax = fig.add_subplot(projection = '3d')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('r')\n",
    "ax.scatter3D(X[:, 0], X[:, 1], r, c = y, s = 40)\n",
    "ax.view_init(elev = 10, azim = 45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the radial kernel to train a model to separate the clusters of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [0.5, 100]):\n",
    "    model = SVC(kernel = 'rbf', C = C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c = y, s = 40)\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels in scikit-learn\n",
    "\n",
    "There are a vast number of kernels that can be used for the Support Vector Machine to find the decision boundaries on non-linear separatable data.\n",
    "\n",
    "With _scikit-learn_ it is possible to use following kernels:\n",
    "* Linear Kernel\n",
    "* Polynomial Kernel\n",
    "* Radial (Gaussian RBF) Kernel\n",
    "* Hyperbolic Tangent (Sigmoid) Kernel\n",
    "* _Precomputed_ Kernel\n",
    "\n",
    "If you choose the _precomputed_ option, you are able to use any (self implemented) kernel for the Support Vector Machine. But keep in mind that you have to transform your _X\\_train_ values with the kernel function before fitting the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Alashwal, H., Safaai Deris, and Razib M. Othman. “A Bayesian Kernel for the Prediction of Protein – Protein Interactions.” International Journal of Computational Intelligence 5, no. 2 (2009): 119-124.\n",
    "\n",
    "* Basak, Jayanta. “A least square kernel machine with box constraints.” International Conference on Pattern Recognition 2008 1 (2008): 1-4.\n",
    "\n",
    "* Boughorbel, S., Jean-Philippe Tarel, and Nozha Boujemaa. “Project-Imedia: Object Recognition.” INRIA – INRIA Activity Reports – RalyX.\n",
    "\n",
    "* Fomel, Sergey. “Inverse B-spline interpolation.” Stanford Exploration Project, 2000.\n",
    "\n",
    "* Genton, Marc G. “Classes of Kernels for Machine Learning: A Statistics Perspective.” Journal of Machine Learning Research 2 (2001) 299-312.\n",
    "\n",
    "* Gunn, S. R. (1998, May). “Support vector machines for classification and regression.” Technical report, Faculty of Engineering, Science and Mathematics School of Electronics and Computer Science.\n",
    "\n",
    "* Hamers B. “Kernel Models for Large Scale Applications”, Ph.D. , Katholieke Universiteit Leuven, Belgium, 2004.\n",
    "\n",
    "* Hastie T., TIbshirani R., Friedman J. \"The elements of statistical learning : data mining, inference, and prediction\". New York, NY: Springer (2009), 417-440.\n",
    "\n",
    "* Hichem Sahbi and François Fleuret. “Kernel methods and scale invariance using the triangular kernel”. INRIA Research Report, N-5143, March 2004.\n",
    "\n",
    "* Hofmann, T., B. Schölkopf, and A. J. Smola. “Kernel methods in machine learning.” Ann. Statist. Volume 36, Number 3 (2008), 1171-1220.\n",
    "\n",
    "* Howley, T. and Madden, M.G. “The genetic kernel support vector machine: Description and evaluation“. Artificial Intelligence Review. Volume 24, Number 3 (2005), 379-395.\n",
    "\n",
    "* Hsuan-Tien Lin and Chih-Jen Lin. “A study on sigmoid kernels for SVM and the training of non-PSD kernels by SMO-type methods.” Technical report, Department of Computer Science, National Taiwan University, 2003.\n",
    "\n",
    "* Huang, Lingkang. “Variable Selection in Multi-class Support Vector Machine and Applications in Genomic Data Analysis.” PhD Thesis, 2008.\n",
    "\n",
    "* James G., Witten D., Hastie T., Tibshirani R. \"An Introduction to Statistical Learning with Applications in R\". New York, NY: Springer (2003), 337-372.\n",
    "\n",
    "* Karatzoglou, A., Smola, A., Hornik, K. and Zeileis, A. “Kernlab – an R package for kernel Learning.”  (2004).\n",
    "\n",
    "* Karatzoglou, A., Smola, A., Hornik, K. and Zeileis, A. “Kernlab – an S4 package for kernel methods in R.” J. Statistical Software, 11, 9 (2004).\n",
    "\n",
    "* Karatzoglou, A., Smola, A., Hornik, K. and Zeileis, A. “R: Kernel Functions.” Documentation for package ‘kernlab’ version 0.9-5.\n",
    "\n",
    "* Li Zhang, Weida Zhou, Licheng Jiao. Wavelet Support Vector Machine. IEEE Transactions on System, Man, and Cybernetics, Part B, 2004, 34(1): 34-39.\n",
    "\n",
    "* Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. “Nonlinear SVMs.” The Stanford NLP (Natural Language Processing) Group.\n",
    "\n",
    "* Micchelli, Charles. Interpolation of scattered data: Distance matrices and conditionally positive definite functions. Constructive Approximation 2, no. 1 (1986): 11-22.\n",
    "\n",
    "* On-Line Prediction Wiki Contributors. “Kernel Methods.” On-Line Prediction Wiki.\n",
    "\n",
    "* Sabri Boughorbel, Jean-Philippe Tarel, and Nozha Boujemaa. “Generalized histogram intersection kernel for image recognition”. Proceedings of the 2005 Conference on Image Processing, volume 3, pages 161-164, 2005.\n",
    "\n",
    "* Shawkat Ali and Kate A. Smith. “Kernel Width Selection for SVM Classification: A Meta-Learning Approach.” International Journal of Data Warehousing & Mining, 1(4), 78-97, October-December 2005.\n",
    "\n",
    "* Vedaldi, A. and Zisserman, A. Efficient Additive Kernels via Explicit Feature Maps. IEEE Transactions on Pattern Recognition and Machine Intelligence, Vol. XX, No. XX, June, 2011.\n",
    "\n",
    "* Weisstein, Eric W. “Positive Semidefinite Matrix.” From MathWorld–A Wolfram.\n",
    "\n",
    "* Wikipedia contributors, “Kernel methods,” Wikipedia, The Free Encyclopedia.\n",
    "\n",
    "* Wikipedia contributors, “Kernel trick,” Wikipedia, The Free Encyclopedia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
