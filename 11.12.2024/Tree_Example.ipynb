{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dstools (absolute path required, please change to your systems settings)\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "path = '/dstools-master/dstools/__init__.py'\n",
    "name = 'dstools'\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(name, path)\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "sys.modules[spec.name] = module\n",
    "spec.loader.exec_module(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from dstools.datasets import iris\n",
    "from dstools.metrics import confusion_matrix, classificationreport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenset vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = iris()\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33,random_state=147)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"gini\", random_state=147,max_depth=4, min_samples_leaf=2)   \n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorhersage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = confusion_matrix(y_pred, y_test, sums=True)\n",
    "confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classificationreport(confmat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(df[\"Class\"].unique())\n",
    "features = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tree.export_graphviz(model,\n",
    "                            out_file=None, \n",
    "                            feature_names=features,\n",
    "                            class_names=labels,\n",
    "                            filled=True,\n",
    "                            rounded=True,  \n",
    "                            special_characters=True)  \n",
    "graph = graphviz.Source(data)  \n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gini\n",
    "1-(0.33**2+0.37**2+0.3**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-((37/67)**2+(30/67)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-((37/40)**2+(3/40)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter\n",
    "\n",
    "https://scikit-learn.org/stable/modules/tree.html#classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion\n",
    "\n",
    "Die Aufteilung bei einem Knoten erfolgt in sklearn entweder mit \n",
    "\n",
    "- Gini Index (*impurity*)\n",
    "- Information Gain (entropy)\n",
    "\n",
    "Die Impurity ist ein Maß, wie homogen die Labels bei einem Node sind. Der Informationsgewinn (*information gain*) verwendet die Entropie als Maß und ein Split erfolgt so, dass der Informationsgewinn möglichst groß wird. Der Gini Index hingegen splittet so, dass die kleinsten Unreinheiten (impurities) entstehen.\n",
    "\n",
    "In der Praxis liefern beide Maßzahlen annähernd gleich gute Ergebnisse, der Gini Index verwendet allerdings keinen Logarithmus zur Berrechnung und ist daher schneller.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitter\n",
    "\n",
    "Hier kann man zwischen **best** und **random** wählen. \n",
    "\n",
    "- best: berechnet die besten Features für den Split, liefert besseres Ergebnis\n",
    "- random: wählt random Features, braucht weniger Rechenzeit, hilft bei Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_depth\n",
    "\n",
    "Die theoretische Tiefe eines Baums ist von der Anzahl der Trainingssamples abhängig (und nicht von der Anzahl der Features, da an den gleichen Features mehrmals gesplittet werden kann). Jedoch führen tiefere Bäume zu **Overfitting**.\n",
    "\n",
    "- None: Der Baum wird solange gesplitted, bis alle Blätter pur sind oder weniger labels als in min_samples_leaf definiert sind haben.\n",
    "\n",
    "> **Wichtig**: Manche Hyperparameter haben Abhängigkeiten untereinander!\n",
    "\n",
    "Allgemein gilt, **je tiefer** ein Baum wird, **desto komplexer** wird das Modell (man hat mehr Splits) und es ist mehr Information über die Trainingsdaten im Modell enthalten. Das ist einer der Gründe für Overfitting!\n",
    "\n",
    "Wenn das Model overfittet, ist die Reduktion der max_depth eine weitere Maßnahme dagegen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
