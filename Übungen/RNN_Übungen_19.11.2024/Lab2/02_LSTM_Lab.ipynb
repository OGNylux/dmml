{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Tutorial 2\n",
    "Thomas Delissen - November 2024\n",
    "\n",
    "Goal of this Tutorial is to get an understanding how LSTM and GRU work, and do some more complicated sequence 2 sequence architectures, such as the bidirectional approach. We will not include the attention mechanism in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Manual calculation of a LSTM Forward pass\n",
    "To fully understand the inner workings of an LSTM Cell, the goal of this first exercise to manually calculate a forward pass through an unrolled graph of an LSTM Model, where we have a single input and a single cell in my LSTM Layer. The Sequence lenght is 2, and we again do a binary classification at the end. Goal is to calculate Y. \n",
    "\n",
    "Note that I will use array notation everywhere, also for single scalars. This way you could extend the code withto also use it for examples where we have larger inputs and more hidden layers (in case this tickles your fancy).\n",
    "\n",
    "I have put the two LSTM timesteps kind of above eachother, so that the diagram remains readable; you should still interpret them as if they are in sequence of course. I also introduced intermediate variables so that you do not need to calculate y in one go. Good luck! \n",
    "\n",
    "![Rolled Out LSTM](lstmTutorial.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:11:52.765838100Z",
     "start_time": "2024-11-21T08:11:52.576492200Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# In this example, we only have 1 hidden unit and 1 input feature, but we still use arrays to ensure compatibility with more complicated architectures. \n",
    "\n",
    "# Input sequence x(0), x(1)\n",
    "x_0 = np.array([[0.5]])\n",
    "x_1 = np.array([[0.8]])\n",
    "\n",
    "# Initial hidden state h(0) and cell state c(0)\n",
    "h_0 = np.array([[0.0]])  \n",
    "c_0 = np.array([[0.0]])  \n",
    "\n",
    "# LSTM cell weights and biases\n",
    "W = np.array([[0.2, 0.4, -0.3, 0.1],  # Weights for input x(t)\n",
    "               [0.1, -0.2, 0.5, 0.3]])  # Weights for previous hidden state h(t-1)\n",
    "B = np.array([[0.1, 0.2, -0.1, 0.0]])  # Biases\n",
    "\n",
    "# Output layer weight and bias\n",
    "Wy = np.array([[1.0]])\n",
    "By = np.array([[0.5]])\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let us do the calculations. As a first step, we will concatenate x_0 and h_0, so they can be mulitplied with W. Then we take the dot product, and add the Bias. Print the result to check how the matrix looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:25:19.453882300Z",
     "start_time": "2024-11-21T08:25:19.447540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined input matrix before split operation at t=0:\n",
      "[[ 0.2   0.4  -0.25  0.05]]\n"
     ]
    }
   ],
   "source": [
    "# First step: Concatenate x(0) and h(0), then apply weights and biases\n",
    "combined_input_0 = np.concatenate((x_0, h_0), axis=1)  # Concatenate x(0) and h(0)\n",
    "combined_result_0 = np.dot(combined_input_0, W) + B  # Apply weights and biases\n",
    "print(\"Combined input matrix before split operation at t=0:\")\n",
    "print(combined_result_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split this matrix (vector) in four parts, each of the numbers represents an input for one of the gates. We can index them from this list when we calculate the intermediate variables, for example by using combined_result[0,1], to get the value for the input gate. Fill in the blanks, using the sigmoid and tanh functions defined before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:25:40.146523200Z",
     "start_time": "2024-11-21T08:25:40.129237700Z"
    }
   },
   "outputs": [],
   "source": [
    "f_0 = sigmoid(combined_result_0[0, 0])  # Forget gate\n",
    "i_0 = sigmoid(combined_result_0[0, 1])  # Input gate\n",
    "c_hat_0 = tanh(combined_result_0[0, 2]) # Candidate gate\n",
    "o_0 = sigmoid(combined_result_0[0, 3])  # Output gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good! With these variables, we can now calculate c_1, and with c_1, we can also calculate h_1. With that, we have the output of the first timestep ready, and can move to the second timestep in the sequence. Elementwise multiplication and Elementwise addition can simply be applied by using the symbols \"*\" and \"+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:25:41.941623700Z",
     "start_time": "2024-11-21T08:25:41.935854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated cell state c(1):\n",
      "[[-0.14662978]]\n",
      "Updated hidden state h(1):\n",
      "[[-0.07461341]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate c(1) based on the gate outputs\n",
    "c_1 = (c_0 * f_0) + (i_0 * c_hat_0)  # Update cell state\n",
    "\n",
    "# Calculate h(1) based on the updated cell state c(1) and output gate\n",
    "h_1 = tanh(c_1) * o_0  # Update hidden state\n",
    "\n",
    "\n",
    "print(\"Updated cell state c(1):\")\n",
    "print(c_1)\n",
    "print(\"Updated hidden state h(1):\")\n",
    "print(h_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work. Now we basically do the same for the second timestep of the LSTM. You might be able to copy a lot of code from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:28:28.072030100Z",
     "start_time": "2024-11-21T08:28:28.037375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined input matrix before split operation at t=1:\n",
      "[[ 0.25253866  0.53492268 -0.3773067   0.05761598]]\n"
     ]
    }
   ],
   "source": [
    "# For the second timestep: Concatenate x(1) and h(1), then apply weights and biases\n",
    "\n",
    "combined_input_1 = np.concatenate((x_1, h_1), axis=1)  # Concatenate x(1) and h(1)\n",
    "combined_result_1 = np.dot(combined_input_1, W) + B  # Apply weights and biases\n",
    "\n",
    "print(\"\\nCombined input matrix before split operation at t=1:\")\n",
    "print(combined_result_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:29:42.122187500Z",
     "start_time": "2024-11-21T08:29:42.104000600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the outputs of the four gates at t=1\n",
    "f_1 = sigmoid(combined_result_1[0, 0])  # Forget gate\n",
    "i_1 = sigmoid(combined_result_1[0, 1])  # Input gate\n",
    "c_hat_1 = tanh(combined_result_1[0, 2])  # Candidate gate\n",
    "o_1 = sigmoid(combined_result_1[0, 3])  # Output gate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:31:02.993578100Z",
     "start_time": "2024-11-21T08:31:02.983265100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate c(2) based on the gate outputs\n",
    "c_2 = (c_1 * f_1) + (i_1 * c_hat_1)  # Update cell state\n",
    "\n",
    "# Calculate h(2) based on the updated cell state and output gate\n",
    "h_2 = tanh(c_2) * o_1  # Update hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, now we have calculated h_2, which is basically the value we will pass to our regular output layer. Now we can directly calculate y. Again, fill in the blanks. This should be similar to the first exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:31:54.360872600Z",
     "start_time": "2024-11-21T08:31:54.339276700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And our final output is:\n",
      "0.5855398664699348\n"
     ]
    }
   ],
   "source": [
    "y = sigmoid(h_2 * Wy + By)  # Compute output\n",
    "\n",
    "print(\"And our final output is:\")\n",
    "print(y[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are the fastest student in the class, and have finished the entire notebook before everyone else, you may come back to this point and do the following bonus exercise: \n",
    "\n",
    "Create a python oneliner that calculates y in one step, without creating intermediate variables. (it is ok to spread out the one-liner over multiple lines for readability, just one \"=\" sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ONLY NEED TO DO THIS IF YOU HAVE TIME LEFT\n",
    "#y = ...\n",
    "print(\"Output should be the same as above:\")\n",
    "print(y[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train an LSTM and a GRU based Model on the example from exercise one\n",
    "In this part of the exercise, you will create your own LSTM RNN as well as a GRU based RNN in Keras, and compare them to the RNNs we had before. \n",
    "\n",
    "We will use the same dataset and preprocessing pipeline from the first exercise, which I will give you here for free; the focus of this tutorial is on RNNs, and not on timeseries data wrangling. \n",
    "\n",
    "Make sure the csv from exercise 1 is in the same folder as this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:32:46.402892400Z",
     "start_time": "2024-11-21T08:32:41.090566400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a3445\\AppData\\Local\\Temp\\ipykernel_17672\\872061735.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['total_rides'] = scaler.fit_transform(train_data[['total_rides']])\n",
      "C:\\Users\\a3445\\AppData\\Local\\Temp\\ipykernel_17672\\872061735.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['total_rides'] = scaler.transform(test_data[['total_rides']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5533\n",
      "1454\n",
      "5519\n",
      "1440\n"
     ]
    }
   ],
   "source": [
    "# Importing and transforming the dataframe from exercise 1\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "file_path = \"CTA_-_Ridership_-_Daily_Boarding_Totals_20241104.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df['service_date'] = pd.to_datetime(df['service_date'], format='%m/%d/%Y')\n",
    "df = df.drop(columns=['day_type', 'bus', 'rail_boardings'])\n",
    "cutoff_date = '2019-12-31'\n",
    "df = df[df['service_date'] <= cutoff_date]\n",
    "df = df.sort_values(by='service_date').reset_index(drop=True)\n",
    "# Split the data into training and test sets based on the specified dates: smaller or equal to 2015-12-31 and bigger than this date. \n",
    "train_data = df[df['service_date'] <= '2015-12-31']\n",
    "test_data = df[(df['service_date'] > '2015-12-31')]\n",
    "\n",
    "# Normalize the training and test data\n",
    "scaler = StandardScaler()\n",
    "train_data['total_rides'] = scaler.fit_transform(train_data[['total_rides']])\n",
    "test_data['total_rides'] = scaler.transform(test_data[['total_rides']])\n",
    "\n",
    "# TRAINING DATA - single\n",
    "X_train_s = []\n",
    "y_train_s = []\n",
    "\n",
    "for i in range(7, len(train_data)):\n",
    "    X_train_s.append(train_data['total_rides'].iloc[i-7:i].values)\n",
    "    y_train_s.append(train_data['total_rides'].iloc[i])\n",
    "\n",
    "# TEST DATA - single\n",
    "X_test_s = []\n",
    "y_test_s = []\n",
    "\n",
    "for i in range(7, len(test_data)):\n",
    "    X_test_s.append(test_data['total_rides'].iloc[i-7:i].values)\n",
    "    y_test_s.append(test_data['total_rides'].iloc[i])\n",
    "\n",
    "# TRAINING DATA - multi\n",
    "X_train_m = []\n",
    "y_train_m = []\n",
    "\n",
    "for i in range(14, len(train_data)-7):\n",
    "    X_train_m.append(train_data['total_rides'].iloc[i-14:i].values)\n",
    "    y_train_m.append(train_data['total_rides'].iloc[i:i+7].values)\n",
    "\n",
    "# TEST DATA - multi\n",
    "X_test_m = []\n",
    "y_test_m = []\n",
    "\n",
    "for i in range(14, len(test_data)-7):\n",
    "    X_test_m.append(test_data['total_rides'].iloc[i-14:i].values)\n",
    "    y_test_m.append(test_data['total_rides'].iloc[i:i+7].values)\n",
    "\n",
    "# BELOW ARE THE VARIABLES WE WILL USE LATER ON\n",
    "# For the single predictions\n",
    "X_train_SinglePred = np.array(X_train_s)\n",
    "y_train_SinglePred = np.array(y_train_s)\n",
    "X_test_SinglePred = np.array(X_test_s)\n",
    "y_test_SinglePred = np.array(y_test_s)\n",
    "\n",
    "# For the series (multi) predictions\n",
    "X_train_MultiPred = np.array(X_train_m)\n",
    "y_train_MultiPred = np.array(y_train_m)\n",
    "X_test_MultiPred = np.array(X_test_m)\n",
    "y_test_MultiPred = np.array(y_test_m)\n",
    "\n",
    "print(len(X_train_SinglePred))\n",
    "print(len(X_test_SinglePred))\n",
    "print(len(X_train_MultiPred))\n",
    "print(len(X_test_MultiPred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared the Data, we can start creating our \"less simple\" neural nets, using LSTMs and GRU. However, since we are using Keras, it is a simple to use these as just importing another layer. Here are the imports we will need: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:33:11.742984Z",
     "start_time": "2024-11-21T08:32:52.414543100Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, SimpleRNN, LSTM, GRU\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU Single Prediction\n",
    "\n",
    "Now let us build our first GRU model to predict the single forecast, based on a sequence. \n",
    "\n",
    "For this we will use the regular sequential API. We will first reshape the inputs again so that it is explicit that we only have one input feature, then feed it into a GRU layer. In the layer of the GRU, we will use 20 GRU cells, activation tanh (because that is the one that is used in the original GRU) and return_sequences is set to False. This is actually the default value for that parameter, so you could also omit it. We won't add any additional layers, just a single celled output without an activation function. \n",
    "\n",
    "You can also already compile it using adam and the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:40:04.225662500Z",
     "start_time": "2024-11-21T08:40:04.197794500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Uni\\5. Semester\\Data Mining\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the X matrices to be compatible with RNN input (samples, time steps, features)\n",
    "X_train_single_gru_lstm = X_train_SinglePred.reshape((X_train_SinglePred.shape[0], X_train_SinglePred.shape[1], 1))\n",
    "X_test_single_gru_lstm = X_test_SinglePred.reshape((X_test_SinglePred.shape[0], X_test_SinglePred.shape[1], 1))\n",
    "\n",
    "# Create the GRU model\n",
    "model_single_gru = Sequential()\n",
    "model_single_gru.add(GRU(20, activation='tanh', return_sequences=False, input_shape=(X_train_single_gru_lstm.shape[1], X_train_single_gru_lstm.shape[2])))\n",
    "model_single_gru.add(Dense(1))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model_single_gru.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for 20 epochs, Batch size 32 and validation split of 0.2. This is the same configuration we used for the SimpleRNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:40:13.020382100Z",
     "start_time": "2024-11-21T08:40:05.945770800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - loss: 0.8618 - val_loss: 0.5440\n",
      "Epoch 2/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4489 - val_loss: 0.2635\n",
      "Epoch 3/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2322 - val_loss: 0.2550\n",
      "Epoch 4/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2013 - val_loss: 0.2487\n",
      "Epoch 5/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2201 - val_loss: 0.2426\n",
      "Epoch 6/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2106 - val_loss: 0.2472\n",
      "Epoch 7/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1956 - val_loss: 0.2321\n",
      "Epoch 8/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1831 - val_loss: 0.2343\n",
      "Epoch 9/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1939 - val_loss: 0.2233\n",
      "Epoch 10/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2049 - val_loss: 0.2251\n",
      "Epoch 11/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1946 - val_loss: 0.2156\n",
      "Epoch 12/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1834 - val_loss: 0.2153\n",
      "Epoch 13/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1822 - val_loss: 0.2154\n",
      "Epoch 14/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1630 - val_loss: 0.2088\n",
      "Epoch 15/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1577 - val_loss: 0.2079\n",
      "Epoch 16/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1877 - val_loss: 0.2080\n",
      "Epoch 17/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2088 - val_loss: 0.2049\n",
      "Epoch 18/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1961 - val_loss: 0.2015\n",
      "Epoch 19/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1729 - val_loss: 0.2028\n",
      "Epoch 20/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1811 - val_loss: 0.1974\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.history.History at 0x145ed49f230>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_single_gru.fit(X_train_single_gru_lstm, y_train_SinglePred, epochs=20, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:40:15.268483700Z",
     "start_time": "2024-11-21T08:40:14.977098200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m46/46\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step\n",
      "Single Prediction RNN Model RMSE: 0.3844938255145831\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate on test data\n",
    "y_single_gru_pred = model_single_gru.predict(X_test_single_gru_lstm)\n",
    "\n",
    "# Calculate RMSE for single prediction RNN\n",
    "rmse_single_gru = root_mean_squared_error(y_test_SinglePred, y_single_gru_pred)\n",
    "print(\"Single Prediction RNN Model RMSE:\", rmse_single_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance might not be too impressive compared to the regular RNN in this example. My assumption is that this architecture is a bit to simple for this problem. Further training, or using more complicated architecture, might change this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM single prediction\n",
    "Let us try to create a single prediction with an LSTM \n",
    "\n",
    "Same approach, we will reuse the reshaped X matrices from the GRU exercise. \n",
    "\n",
    "Again, use a LSTM layer with 20 LSTM cells, tanh activation function and return sequences = false.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:41:11.391611100Z",
     "start_time": "2024-11-21T08:41:11.364747800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Uni\\5. Semester\\Data Mining\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create the LSTM model\n",
    "model_single_lstm = Sequential()\n",
    "model_single_lstm.add(LSTM(20, activation='tanh', return_sequences=False, input_shape=(X_train_single_gru_lstm.shape[1], X_train_single_gru_lstm.shape[2])))\n",
    "model_single_lstm.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model_single_lstm.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, train the model for 20 epochs, Batch size 32 and validation split of 0.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:41:29.859050800Z",
     "start_time": "2024-11-21T08:41:23.275041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - loss: 0.9753 - val_loss: 0.4068\n",
      "Epoch 2/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2980 - val_loss: 0.2934\n",
      "Epoch 3/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2541 - val_loss: 0.2807\n",
      "Epoch 4/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2483 - val_loss: 0.2743\n",
      "Epoch 5/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2139 - val_loss: 0.2654\n",
      "Epoch 6/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2226 - val_loss: 0.2742\n",
      "Epoch 7/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2407 - val_loss: 0.2528\n",
      "Epoch 8/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2350 - val_loss: 0.2496\n",
      "Epoch 9/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2151 - val_loss: 0.2428\n",
      "Epoch 10/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2087 - val_loss: 0.2378\n",
      "Epoch 11/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2036 - val_loss: 0.2403\n",
      "Epoch 12/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2083 - val_loss: 0.2336\n",
      "Epoch 13/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2189 - val_loss: 0.2289\n",
      "Epoch 14/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2297 - val_loss: 0.2283\n",
      "Epoch 15/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1863 - val_loss: 0.2295\n",
      "Epoch 16/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2126 - val_loss: 0.2244\n",
      "Epoch 17/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.2079 - val_loss: 0.2226\n",
      "Epoch 18/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1888 - val_loss: 0.2179\n",
      "Epoch 19/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1764 - val_loss: 0.2156\n",
      "Epoch 20/20\n",
      "\u001B[1m139/139\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.1823 - val_loss: 0.2115\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.history.History at 0x145f13e6c00>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_single_lstm.fit(X_train_single_gru_lstm, y_train_SinglePred, epochs=20, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:41:45.481611800Z",
     "start_time": "2024-11-21T08:41:45.230636900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m46/46\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step\n",
      "Single Prediction RNN Model RMSE: 0.38247860472619744\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate on test data\n",
    "y_single_lstm_pred = model_single_lstm.predict(X_test_single_gru_lstm)\n",
    "\n",
    "# Calculate RMSE for single prediction RNN\n",
    "rmse_single_lstm = root_mean_squared_error(y_test_SinglePred, y_single_lstm_pred)\n",
    "print(\"Single Prediction RNN Model RMSE:\", rmse_single_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, the LSTM also did not seem to work very well for this example. However, hopefully you did notice how easy it was to exchange the layers in these examples; Keras and Tensorflow take care of the complications behind the scenes, we just need to include a single model. \n",
    "\n",
    "Let us try with the sequence to sequence problem we defined before, perhaps the LSTM will perform better there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Series prediction\n",
    "We will now evaluate the LSTM on the series model, to see how it works. \n",
    "\n",
    "Like before, we want to predict the next 7 values, based on the last 14. This is a sequence to sequence task, and will be modeled by using the encoder - decoder architecture. This will follow the same pattern as the first exercise, except this time we will use LSTMs instead of SimpleRNNs. \n",
    "\n",
    "Start by defining the encoder. \n",
    "\n",
    "One difference with the regular RNN is that we don't connect the decoder and the encoder over the output of the encoder, but we connect both the hidden state as well as the context from the encoder.\n",
    "\n",
    "Fill in the blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:50:24.745187400Z",
     "start_time": "2024-11-21T08:50:24.726091600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the input shape for the encoder (14 days of data, 1 feature)\n",
    "encoder_inputs = Input(shape=(14, 1))\n",
    "\n",
    "# Encoder LSTM RNN: Process the input sequence and output the final hidden state as the output\n",
    "encoder_lstm = LSTM(20, activation='tanh', return_sequences=False, return_state=True)\n",
    "encoder_output, state_h, state_c = encoder_lstm(encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the decoder. We will mostly reuse the code from before, except that we will use LSTM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:46:10.178104200Z",
     "start_time": "2024-11-21T08:46:10.148554400Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(7, 1))  # Here we will input the 7 zeroes\n",
    "\n",
    "# I will also already provide dummy values for the training data\n",
    "decoder_input_train = tf.zeros((X_train_MultiPred.shape[0], 7, 1))\n",
    "\n",
    "# LSTM RNN Layer of decoder: \n",
    "decoder_lstm = LSTM(20, activation='tanh', return_sequences=True, return_state=False)\n",
    "decoder_lstm_outputs = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c]) # in this case, we forward the hidden state and state c from the encoder to the decoder\n",
    "# Final dense layer of decoder. \n",
    "decoder_dense = Dense(1) # this defines the output layer of the network\n",
    "decoder_final_outputs = decoder_dense(decoder_lstm_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, same as in the first exercise, we define the Model as the combination of the encoder and decoder. To define it, we need to provide the inputs and outputs of the elements we just defined. \n",
    "\n",
    "Inputs: \n",
    "- Encoder inputs\n",
    "- Decoder inputs (7 zeroes)\n",
    "\n",
    "Outputs: \n",
    "- Decoder output (7 predictions)\n",
    "\n",
    "Define the model, then compile it as before, with optimizer adam and loss function mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:47:19.276372Z",
     "start_time": "2024-11-21T08:47:19.257766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model with encoder and decoder\n",
    "model_series_lstm = Model([encoder_inputs, decoder_inputs], decoder_final_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model_series_lstm.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train the model, using 20 Epochs, Batch size 32, validation split 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:48:12.046358800Z",
     "start_time": "2024-11-21T08:47:56.979773800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Uni\\5. Semester\\Data Mining\\venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_13', 'keras_tensor_18']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.9717 - val_loss: 0.7915\n",
      "Epoch 2/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.8003 - val_loss: 0.7241\n",
      "Epoch 3/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.6579 - val_loss: 0.5471\n",
      "Epoch 4/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.4568 - val_loss: 0.4378\n",
      "Epoch 5/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.3804 - val_loss: 0.3981\n",
      "Epoch 6/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.3363 - val_loss: 0.3726\n",
      "Epoch 7/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.2989 - val_loss: 0.3572\n",
      "Epoch 8/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.2895 - val_loss: 0.3494\n",
      "Epoch 9/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.2752 - val_loss: 0.3337\n",
      "Epoch 10/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.2641 - val_loss: 0.3244\n",
      "Epoch 11/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.2403 - val_loss: 0.3181\n",
      "Epoch 12/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - loss: 0.2426 - val_loss: 0.3121\n",
      "Epoch 13/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - loss: 0.2366 - val_loss: 0.3029\n",
      "Epoch 14/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - loss: 0.2375 - val_loss: 0.2954\n",
      "Epoch 15/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - loss: 0.2321 - val_loss: 0.2935\n",
      "Epoch 16/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - loss: 0.2231 - val_loss: 0.2883\n",
      "Epoch 17/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - loss: 0.2110 - val_loss: 0.2829\n",
      "Epoch 18/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - loss: 0.2148 - val_loss: 0.2801\n",
      "Epoch 19/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - loss: 0.2050 - val_loss: 0.2809\n",
      "Epoch 20/20\n",
      "\u001B[1m138/138\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - loss: 0.2100 - val_loss: 0.2729\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.history.History at 0x145f27fd2b0>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_series_lstm.fit([X_train_MultiPred, decoder_input_train], y_train_MultiPred, epochs=20, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T08:48:51.338167600Z",
     "start_time": "2024-11-21T08:48:51.071348500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m37/45\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 1ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Uni\\5. Semester\\Data Mining\\venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_13', 'keras_tensor_18']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m45/45\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n",
      "Series Prediction LSTM Model RMSE: 0.44198150402846453\n"
     ]
    }
   ],
   "source": [
    "# Generate dummy values for the test data: \n",
    "decoder_input_test = tf.zeros((X_test_MultiPred.shape[0], 7, 1))\n",
    "\n",
    "# Predict and evaluate on test data\n",
    "y_series_lstm_pred = model_series_lstm.predict([X_test_MultiPred, decoder_input_test])\n",
    "\n",
    "# Remove the last dimension to get the desired shape\n",
    "y_series_lstm_pred = np.squeeze(y_series_lstm_pred, axis=-1)\n",
    "\n",
    "# Calculate RMSE for single prediction RNN\n",
    "rmse_series_lstm = root_mean_squared_error(y_series_lstm_pred, y_test_MultiPred)\n",
    "print(\"Series Prediction LSTM Model RMSE:\", rmse_series_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you also trained a LSTM model on the series data now. In my case, the performance was worse than for the regular RNN. Perhaps this is caused by the problem definition, perhaps we could improve performance by changing the architecture of the LSTM model. \n",
    "\n",
    "For the purpose of this tutorial, that doesn't matter, the goal was to create a working encoder - decoder architecture, and that seems to work. Combined with proper time series modeling (which will be taught later), LSTMs can be powerful tools. They also excel with NLP Tasks, as you will see in the next semester. \n",
    "\n",
    "Feel free to experiment and extend this model for your own projects. \n",
    "\n",
    "You are now ready to try and tackle the RNN / LSTM Assignment. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
