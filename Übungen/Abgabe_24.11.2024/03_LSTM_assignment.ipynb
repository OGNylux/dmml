{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Assignment RNN / LSTM DMML\n",
    "Thomas Delissen - November 2024\n",
    "\n",
    "This assignment is based on the Blog from Andrej Karpathy: https://karpathy.github.io/2015/05/21/rnn-effectiveness/ \n",
    "\n",
    "That Website might give you some useful hints. Note that you cannot copy the code from that Github, since the assignment you will have to do is to create an LSTM using Keras instead of implementing a manual RNN. \n",
    "\n",
    "The goal will be to create an LSTM using Keras that can generate long sequences of characters, hopefully resembling the input text. So this will be a NLP Sequence to Sequence Model that can generate single characters. \n",
    "\n",
    "This exercise uses the file tinyshakespeare_karpathy.txt . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:30:29.817381200Z",
     "start_time": "2024-11-21T14:30:15.219145300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1115394\n",
      "Vocab size: 65\n",
      "Available Characters: ['k', 'C', 'U', 'L', 'x', 'D', 'e', 'w', 'm', 'M', 'V', 'p', 'Y', '&', 'Q', 'S', '3', 'J', 'u', 'H', 'X', 'o', ',', 'q', 'I', 'A', 'B', '!', 'W', ';', 'f', 's', 'c', 'n', '$', 'Z', 'l', 'T', 'F', 'O', 'v', 'R', 't', '?', 'j', 'K', '-', 'r', 'E', 'a', 'g', 'h', 'y', \"'\", '\\n', 'i', 'b', 'G', '.', 'N', 'z', ':', 'P', ' ', 'd']\n"
     ]
    }
   ],
   "source": [
    "# The following libraries might come in handy\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# data I/O - code copied from Andrej Karphaty github\n",
    "data = open('tinyshakespeare_karpathy.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "print(\"Data size: \" + str(data_size))\n",
    "print(\"Vocab size: \" + str(vocab_size))\n",
    "print(\"Available Characters: \" + str(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries char_to_ix and ix_to_char can be used to convert the character to a number and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:30:33.384884400Z",
     "start_time": "2024-11-21T14:30:33.376303200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "U\n"
     ]
    }
   ],
   "source": [
    "print(char_to_ix[\"E\"])\n",
    "print(ix_to_char[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have read in the data in your notebook. Your assignment is the following: \n",
    "\n",
    "1. Prepare the data for training an LSTM\n",
    "\n",
    "We will take an input sequence lenght of 40 characters. Proposed solution: Create an X and a y variable of type numpy array, where each row in X should be 40 characters, encoded as numbers, and each row in y should contain the next character, also encoded as a number. Once the numpy arrays have been created, use the to_categorical function from keras to one-hot-encode the arrays. We do not need to split off a test set, since we will not evaluate the LSTM, just use it to generate text. \n",
    "\n",
    "2. Design an LSTM for the training data you just created, compile it and train it. \n",
    "\n",
    "You can decide yourself on the architecture, can be simple or complex, as you like. Proposal: Your final layer will most likely be a multiclass classification of size vocab_size, so you should use the appropriate activation function for that. \n",
    "\n",
    "3. Create a function (def) that can iteratively call your RNN / LSTM\n",
    "\n",
    "The function should expect as an input an initial (40) character string, so that you can generate the first next character. A second parameter of the function should be the number of tokens you like to generate. Inside the function there should be a for loop that iteratively calls your model. Append your generated tokens to the initial string and feed your string into your LSTM iteratively to generate the next token. Output the completely generated text. \n",
    "\n",
    "4. Demonstrate your function\n",
    "\n",
    "Call your function with a string you defined yourself, and let it generate 200 characters. Hopefully, it resembles syntactically correct English. \n",
    "\n",
    "For Reference, here is a text that my model generated: \n",
    "\n",
    "____________________________\n",
    "First Citizen:\n",
    "Before we proceed any further with the cours.\n",
    "\n",
    "PETRUCHIO:\n",
    "What shall be the death the strength the strength.\n",
    "\n",
    "KING RICHARD III:\n",
    "The shall be the death the strength the strength.\n",
    "\n",
    "KING RICHARD III:\n",
    "The shall be the death the s\n",
    "\n",
    "_____________________________\n",
    "\n",
    "That is it! You are allowed to use parts from the Github I recommended, and are also allowed to use ChatGPT. But, very important: You must be able to explain your code, so you can only use stuff from ChatGPT that you understand. The model must be created using Keras and Tensorflow. It is allowed to deviate from the detailed instructions from above, as long as you can explain why you did that and as long as your model is able to generate text. In case you do not manage to complete the entire assignment (perhaps only part 1 and 2), please still submit your work, you will get points for it."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1) Prepare the data for training an LSTM\n",
    "We will take an input sequence lenght of 40 characters. Proposed solution: Create an X and a y variable of type numpy array, where each row in X should be 40 characters, encoded as numbers, and each row in y should contain the next character, also encoded as a number. Once the numpy arrays have been created, use the to_categorical function from keras to one-hot-encode the arrays. We do not need to split off a test set, since we will not evaluate the LSTM, just use it to generate text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:40:41.613417300Z",
     "start_time": "2024-11-21T14:40:37.500951500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115353, 40)\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "\n",
    "seq_length = 40\n",
    "for char in range(seq_length, len(data) - 1):\n",
    "    row = data[char - seq_length: char]\n",
    "    predict_char = data[char]\n",
    "    predict_char_encoded = char_to_ix[predict_char]\n",
    "    row_encoded = [char_to_ix[c] for c in row]\n",
    "\n",
    "    X.append(row_encoded)\n",
    "    y.append(predict_char_encoded)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X.shape)\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2) Design an LSTM for the training data you just created, compile it and train it.\n",
    "\n",
    "You can decide yourself on the architecture, can be simple or complex, as you like. Proposal: Your final layer will most likely be a multiclass classification of size vocab_size, so you should use the appropriate activation function for that."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Uni\\5. Semester\\Data Mining\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Design an LSTM for the training data you just created, compile it and train it. You can decide yourself on the architecture, can be simple or complex, as you like. Proposal: Your final layer will most likely be a multiclass classification of size vocab_size, so you should use the appropriate activation function for that.\n",
    "\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# we use categorical_crossentropy as loss function, since we are dealing with a multiclass classification problem and not a regression problem\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T14:46:16.069855800Z",
     "start_time": "2024-11-21T14:46:15.983305300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m 4505/27884\u001B[0m \u001B[32m━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m22:54\u001B[0m 59ms/step - loss: 2.5479"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-11-21T14:52:32.161068100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
